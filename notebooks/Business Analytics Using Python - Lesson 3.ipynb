{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red>Introduction to Business Analytics:<br>Using Python for Better Business Decisions</font>\n",
    "=======\n",
    "<br>\n",
    "    <center><img src=\"http://dataanalyticscorp.com/wp-content/uploads/2018/03/logo.png\"></center>\n",
    "<br>\n",
    "Taught by: \n",
    "\n",
    "* Walter R. Paczkowski, Ph.D. \n",
    "\n",
    "    * My Affliations: [Data Analytics Corp.](http://www.dataanalyticscorp.com/) and [Rutgers University](https://economics.rutgers.edu/people/teaching-personnel)\n",
    "    * [Email Me With Questions](mailto:walt@dataanalyticscorp.com)\n",
    "    * [Learn About Me](http://www.dataanalyticscorp.com/)\n",
    "    * [See My LinkedIn Profile](https://www.linkedin.com/in/walter-paczkowski-a17a1511/)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue> Lesson \\#3:<br>Predictive Modeling: Introduction to Machine Learning </font>\n",
    "\n",
    "In this lesson, you will learn:\n",
    "\n",
    "1. to divide your data set into training and testing sets;\n",
    "2. estimate *OLS* and Logistic models; and\n",
    "3. grow decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> Reset the Data from Lesson 1 </font>\n",
    "\n",
    "Resetting the data will ensure that the work you did in Lesson 1 is available in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Load packages\n",
    "##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "##\n",
    "## Import the data.  The parse_dates argument says to \n",
    "## treat Tdate as a date object.\n",
    "##\n",
    "file = r'../Data/furniture/final data files/orders.csv'\n",
    "df_orders = pd.read_csv( file, parse_dates = [ 'Tdate' ] )\n",
    "pd.set_option('display.max_columns', 8)\n",
    "##\n",
    "## Initial Calculations\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ 'Tdisc' ] = df_orders[ x ].sum( axis = 1 )\n",
    "##\n",
    "df_orders[ 'Pprice' ] = df_orders.Lprice*( 1 - df_orders.Tdisc )\n",
    "##\n",
    "df_orders[ 'Rev' ] = df_orders.Usales * df_orders.Pprice\n",
    "##\n",
    "df_orders[ 'Con' ] = df_orders.Rev - df_orders.Mcost\n",
    "df_orders[ 'CM' ] = df_orders.Con/df_orders.Rev\n",
    "##\n",
    "df_orders[ 'netRev' ] = ( df_orders.Usales - df_orders.returnAmount )*df_orders.Pprice\n",
    "df_orders[ 'lostRev' ] = df_orders.Rev - df_orders.netRev\n",
    "##\n",
    "##\n",
    "## Import a second DataFrame on the customers\n",
    "##\n",
    "file = r'../Data/furniture/final data files/customers.csv'\n",
    "df_cust = pd.read_csv( file )\n",
    "##\n",
    "## Do an inner join using CID as the link\n",
    "##\n",
    "df = pd.merge( df_orders, df_cust, on = 'CID' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> Steps for Predictive Modeling </font>\n",
    "\n",
    "There are three steps for predictive modeling:\n",
    "\n",
    "1. Split your data into two parts: Training and Testing;\n",
    "2. Train a model with the training data set; and\n",
    "3. Test the trained model with the testing data set.\n",
    "\n",
    "The following sections will illustrate these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Steps for Predictive Modeling: Train/Test Split Data </font>\n",
    "\n",
    "The data are split into two parts using *sklearn*.  Each part has a *X* variable array and a *y* vector (The upper and lower cases are conventional).  The *X* array is a Pandas DataFrame of the *X* variables.  The *y* vector is a Pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import train_test_split package\n",
    "##\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create the X and y data for splitting\n",
    "##\n",
    "y = df[ 'Usales' ]\n",
    "x = [ 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc', 'Region', 'buyerRating' ]\n",
    "X = df[ x ]\n",
    "##\n",
    "## Split the data.  The default is 3/4 train.\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.25,\n",
    "                                                    random_state = 42 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The dependent and independent variables need to be separated from the main DataFrame before the train/test split can be done.  The index from the main DataFrame is preserved.  The first three lines of code do this.  The *train_test_split* function randomly divides the data, keeping the indexes aligned.  The *random_state = 42* argument sets the random seed.  Four data sets are returned which are (in order): *X_train, X_test, y_train*, and *y_test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data\n",
    "##\n",
    "print(\"Sample sizes: \\nX: {}, y: {}\\n\".format( X_train.shape[0], y_test.shape[0] ) )\n",
    "print( 'Training Data: \\n{}'.format( X_train.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Testing Data: \\n{}'.format( y_test.head() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Note the indexes for the training and testing data sets.  These are the same as the main DataFrame, *df*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y training data for \n",
    "## model training.  Do an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable: Usales\n",
    "##\n",
    "yy = pd.DataFrame( { 'Usales':y_train } )\n",
    "train = yy.merge( X_train, left_index = True, right_index = True )\n",
    "print( 'Training Data Set:\\n\\n{}'.format( train.head() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *X* and *Y* training data sets are merged on the indexes.  Recall that the index were preserved when the *y* and *X* data sets were created.  This is why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y testing data sets for predicting.\n",
    "## Use an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable Usales.\n",
    "##\n",
    "yy = pd.DataFrame( { 'Usales':y_test } )\n",
    "test = yy.merge( X_test, left_index = True, right_index = True )\n",
    "print( 'Testing Data Set:\\n\\n{}'.format( test.head() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add log Usales and log Pprice to the training data\n",
    "## The log is based on the Numpy function log1p\n",
    "## Note: log1p( x ) = log( 1 + x )\n",
    "##\n",
    "train[ 'log_Usales' ] = np.log1p( train.Usales )\n",
    "train[ 'log_Pprice' ] = np.log1p( train.Pprice )\n",
    "print( 'Training Data Set:\\n\\n{}'.format( train.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Training Data Set Shape:\\n {}'.format( train.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Logged terms are added because the Data Visualization showed that logs induce normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat for the testing data\n",
    "##\n",
    "test[ 'log_Usales' ] = np.log1p( test.Usales )\n",
    "test[ 'log_Pprice' ] = np.log1p( test.Pprice )\n",
    "print( 'Testing Data Set:\\n\\n{}'.format( test.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Testng Data Set Shape:\\n {}'.format( test.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Steps for Predictive Modeling: Training a Model </font>\n",
    "\n",
    "I will cover three predictive models:\n",
    "\n",
    "1. *OLS*\n",
    "2. Logit\n",
    "3. Decision trees\n",
    "\n",
    "Which one is used depends on the dependent variable which can be continuous or discrete.  There are three cases corresponding to the two predictive models:\n",
    "\n",
    "- Case I: Continuous Dependent Variable -- *OLS* Regression\n",
    "- Case II: Binary Dependent Variable -- Logistic Regression\n",
    "- Case III: Constants -- Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Case I: Continuous Dependent Variable -- *OLS* Regression </font>\n",
    "\n",
    "Model unit sales as a function of the pocket price to get a price elasticity.  Recall that you are using log terms and that the estimated coefficient for log price is the elasticity.\n",
    "\n",
    "**Recommendation**:  Use formulas to specify the model.  You need the *statsmodels.formula* api for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "## OLS\n",
    "##\n",
    "## For modeling, notice the new import command for\n",
    "## the formula API and the summary option\n",
    "##\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf \n",
    "##\n",
    "## There are four steps for estimatng a model:\n",
    "##   1. define a formula (i.e., the specific model to estimate)\n",
    "##   2. instantiate the model (i.e., specify it)\n",
    "##   3. fit the model\n",
    "##   4. summarize the fitted model\n",
    "##\n",
    "## ===> Step 1: Define a formula\n",
    "##\n",
    "## The formula uses a “~” to separate the left-hand side from the right-hand side\n",
    "## of a model and a “+” to add columns to the right-hand side.  A “-” sign (not \n",
    "## used here) can be used to remove columns from the right-hand side (e.g.,\n",
    "## remove or omit the constant term which is always included by default). \n",
    "##\n",
    "formula = 'log_Usales ~ log_Pprice + Ddisc + Odisc + Cdisc + Pdisc + C( Region )'\n",
    "##\n",
    "## Since Region is categorical, you must create dummies for the regions.  You\n",
    "## do this using 'C( Region )' to indicate that Region is categorical.\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model\n",
    "##\n",
    "mod = smf.ols( formula, data = train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model\n",
    "##      Recommendation: number your models\n",
    "##\n",
    "reg01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model\n",
    "##\n",
    "print( reg01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The modeling follows four steps as shown above.  Regardless of the software you might use, these same four steps are followed.  Some software combines them, others require explicit statement.  This is what statsmodels requires.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The price elasticity is the coefficient for the logged price variable (i.e., log_Pprice): -1.7.  If price falls by 1\\%, unit sales rise by 1.7\\%.  This indicates that blinds are highly elastic.  This should be expected since furniture is a competitive business and blinds are very competitive.  Revenue will also change.  If price fall, revenue will increase.  The amount revenue will increase (in percentage terms) is given by $1 + elasticity$.  So for a 1\\% fall in price, revenue will rise 0.7\\% (= $1 + [-1.7]$). \n",
    "\n",
    "The discounts and regions seem to have no effect, but this can be tested as we'll do below.  Also note that the $R^2$ is 0.20 which is very low.  \n",
    "\n",
    "The Jarque-Bera Test is a test for normality of the disturbance term.  It is a test of the \"goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. $\\ldots$ The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero.  $\\ldots$ If it is far from zero, it signals the data do not have a normal distribution.\"  So the Null Hypothesis is $H_O: Normality$.  (Source: <a href=\"https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test\" target=\"_parent\">see here</a>)  In this case, the Null is rejected.  The Omnibus Test is an alternative test of normality with the same Null.  It also indicates that the Null must be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> Exercises </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Exercise \\#3.1 </font>\n",
    "\n",
    "Estimate a new *OLS* model by adding the buyer rating to the above model. Interpret your results.  Is the buyer rating important for sales?\n",
    "\n",
    "**Hint**: Buyer rating is categorical so you have to create dummies for the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Analyzing the Results </font>\n",
    "\n",
    "Quantities of interest can be extracted directly from the fitted model. Type *dir(results)* for a full list.\n",
    "\n",
    "Since the product manager wanted to know about a region effect, you should do an F-test of all the coefficients for the regions to determine if they are all zero, meaning that the dummies as a group do nothing.  This is a <u>joint</u> test of significance.  The test statistic is:\n",
    "\n",
    "$F_C = \\dfrac{\\left(SSR_U - SSR_R\\right)/(df_U - df_R)}{SSE_U/(n - p - 1)} = \\dfrac{\\left(SSE_R - SSE_U\\right)/(df_U - df_R)}{SSE_U/(n - p - 1)}$\n",
    "\n",
    "where \"U\" indicates the *unrestricted* or *full* model with the Region dummies and \"R\" indicates the *restricted* model without the Region dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify the joint (Null) hypothesis that the regions are the same;\n",
    "## i.e., there is no region effect.\n",
    "##\n",
    "hypothesis = ' ( C(Region)[T.Northeast] = 0, C(Region)[T.South] = 0, C(Region)[T.West] = 0 ) '\n",
    "##\n",
    "## Run and print an F-test \n",
    "##\n",
    "f_test = reg01.f_test( hypothesis )\n",
    "f_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Notice that there are only three regions specified even though there are four: one is omitted as the base.  Also notice that the three hypotheses are specified as *C(Region)[T.XX] = 0* where *XX* is the region name.\n",
    "\n",
    "**_Output Interpretation_**\n",
    "\n",
    "The returned values for the F-test are, in order:\n",
    "\n",
    "1. The F-Statistic value\n",
    "2. The p-value for the F-Statistic\n",
    "3. The F-Statistic's denominator degrees-of-freedom\n",
    "4. The F-Statistic's numerator degrees-of-freedom\n",
    "\n",
    "**_Interpetation_**\n",
    "\n",
    "The Null Hypothesis is that there is no region effect.  The p-value is 0.32 so the Null Hypothesis is not rejected: there is no Region effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat the F-test for the discounts\n",
    "##\n",
    "hypothesis = ' ( Ddisc = 0, Odisc = 0, Cdisc = 0, Pdisc = 0 ) '\n",
    "##\n",
    "## Run and print an F-test \n",
    "##\n",
    "f_test = reg01.f_test( hypothesis )\n",
    "f_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The hypothesis statement does not have the discount names as *C(Ddis)* etc. because they are quantitative variables, not categorical variables like *Region*. \n",
    "\n",
    "The Null Hypothesis is that there is no difference among the discounts; they all have zero effect on unit sales.  Notice that the p-value is 0.34.  So the Null Hypothesis that the discounts all have the same effect is not rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue> Exercises </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Exercise \\#3.2 </font>\n",
    "\n",
    "Test the Null Hypothesis that all the buyer rating estimated parameters are zero.  That is, there is no difference among the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for multicollinearity -- a linear relationship among the variables.  Use the *variance inflation factor* (*VIF*).  A rule-of-thumb is that any $VIF > 10$ indicates a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the design matrix to eliminate the first column of 1s\n",
    "## the iloc method says to find the location of columns based on \n",
    "## their integer locations (i.e., 0, 1, 2, etc.)\n",
    "## the term in brackets says to find all rows (the : ) and all \n",
    "## columns from the first to the end (1: )\n",
    "##\n",
    "## Create the correlation matrix\n",
    "##\n",
    "x = reg01.model.data.orig_exog.iloc[ :, 1: ] \n",
    "corr_matrix = x.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Graph the correlation matrix\n",
    "##\n",
    "sns.heatmap( corr_matrix ).set_title( 'Heatmap of the Correlation Matrix' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## A fancy version of the heatmap\n",
    "## Based on: https://stackoverflow.com/questions/39409866/correlation-heatmap\n",
    "##\n",
    "cmap = sns.diverging_palette( 5, 250, as_cmap = True )\n",
    "##\n",
    "corr_matrix.style.background_gradient( cmap, axis=1 ).set_precision( 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate VIFs\n",
    "## The VIFs are the diagonal elements of the inverted correlation\n",
    "## matrix of the independent variables.\n",
    "##\n",
    "## Subset the design matrix to eliminate the first column of 1s.\n",
    "## The iloc method says to find the location of columns based on their \n",
    "## integer locations (i.e., 0, 1, 2, etc.) the term in brackets says \n",
    "## to find all rows (the : ) and all columns from the first to the end (1: ).\n",
    "##\n",
    "## Create the correlation matrix\n",
    "##\n",
    "x = reg01.model.data.orig_exog.iloc[ :, 1: ]\n",
    "corr_matrix = x.corr()\n",
    "##\n",
    "## Invert the correlation matrix and extract the main diaginal\n",
    "##\n",
    "vif = np.diag( np.linalg.inv( corr_matrix ) ) \n",
    "##\n",
    "## Zip the variable names and the VIFs\n",
    "##\n",
    "indepvars = [ i for i in x.columns ]\n",
    "xzip = zip( indepvars, vif ) \n",
    "##\n",
    "## Display the zip matrix.  First import a needed function:\n",
    "##\n",
    "from statsmodels.compat import lzip\n",
    "lzip( xzip )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The *VIF*s are all below 10 so there is no problem.  $VIF > 10$ is a rule-of-thumb for indicating multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Portfolio\n",
    "\n",
    "This is a nice way to summarize the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import some packags\n",
    "##\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from statsmodels.stats.api import anova_lm\n",
    "##\n",
    "## Create a variable to hold the model names; this is a list.\n",
    "## Note: the range() function specifies 1 - 2 but the \"2\" is\n",
    "## not included.\n",
    "##\n",
    "model_names = [ 'Model ' + str( i ) for i in range( 1, 2 ) ]\n",
    "##\n",
    "## Create a variable to hold the statistics to print; this is a dictionary.\n",
    "##\n",
    "info_dict = { '\\nn': lambda x: \"{0:d}\".format( int( x.nobs ) ),\n",
    "              'R2 Adjusted': lambda x: \"{:0.3f}\".format( x.rsquared_adj ),\n",
    "              'AIC': lambda x: \"{:0.2f}\".format( x.aic ),\n",
    "              'F': lambda x: \"{:0.2f}\".format( x.fvalue ),\n",
    "}\n",
    "##\n",
    "## Create the portfolio summary table.\n",
    "##\n",
    "summary_table = summary_col( [ reg01 ],\n",
    "            float_format = '%0.2f',\n",
    "            model_names = model_names,\n",
    "            stars = True, \n",
    "            info_dict = info_dict \n",
    ")\n",
    "summary_table.add_title( 'Summary Table for Living Room Blinds Sales' )\n",
    "print( summary_table )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Predicting with the Model </font>\n",
    "\n",
    "Predict unit sales.  Recognize that sales are in (natural) log terms.  You will convert back to unit sales in \"normal\" terms later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate predicted log of unit sales, the dependent variable.\n",
    "##\n",
    "## Note: the inverse of the log is needed; use np.expm1( x )\n",
    "## since log1p was used: np.expm1 = exp(x) - 1.\n",
    "##\n",
    "log_pred = reg01.predict( test )\n",
    "y_pred = np.expm1( log_pred )\n",
    "##\n",
    "##\n",
    "## Combine into one temporary DataFrame for convenience\n",
    "##\n",
    "tmp = pd.DataFrame( { 'y_test':y_test, 'y_logPred':log_pred, 'y_pred':y_pred } )\n",
    "tmp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the sklearn metrics function *r2_score* to check the fit of actual vs. predicted values.  From the sklearn User Guide:\n",
    "\n",
    "\"*The r2_score function computes R², the coefficient of determination. It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Import the r2_score function from the sklearn metrics package\n",
    "##\n",
    "from sklearn.metrics import r2_score\n",
    "##\n",
    "## Display the r2 score.  But first drop any NaN data.\n",
    "##\n",
    "tmp.dropna( inplace = True )\n",
    "print( 'r2 Score:\\n {}'. format( round( r2_score( tmp.y_test, tmp.y_pred), 3 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also graph the actual vs predicted values.  Sometimes, however, the number of data points is too large to plot so a random sample may be needed.  This is our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Draw a random sample of 500 observations without replacement\n",
    "## from the tmp DataFrame.\n",
    "##\n",
    "smpl = tmp.sample( n = 500, replace = False, random_state = 1 )\n",
    "##\n",
    "## Plot the data\n",
    "##\n",
    "ax = sns.regplot( x = 'y_test', y = 'y_pred', scatter = True, data = smpl )\n",
    "ax.set( title = 'Actual vs Predicted Units Sales\\nRandom Sample of 500', \n",
    "       ylabel = 'Predicted Sales', xlabel = 'Actual Sales' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict unit sales for different settings of the variables.  This is *scenario* or *what-if* analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify scenario values to use for prediction\n",
    "##\n",
    "## Create a dict\n",
    "##\n",
    "data = {\n",
    "         'Pprice': [ 2.50 ],\n",
    "         'Ddisc': [ 0.03 ],\n",
    "         'Odisc': [ 0.05 ],\n",
    "         'Cdisc': [ 0.03 ],\n",
    "         'Pdisc': [ 0.03 ],\n",
    "         'Region': [ 'West' ]\n",
    "        }\n",
    "##\n",
    "## Create a DataFrame using the dict\n",
    "##\n",
    "scenario = pd.DataFrame.from_dict( data )\n",
    "##\n",
    "## Insert a log price column after the Pprice variable\n",
    "##\n",
    "scenario.insert( loc = 1, column = 'log_Pprice',\n",
    "                value = np.log1p( scenario.Pprice ) )\n",
    "##\n",
    "## Display the settings and the predicted unit sales\n",
    "##\n",
    "print( 'Scenario settings:\\n{}'.format( scenario ) )\n",
    "##\n",
    "## Create a pediction\n",
    "##\n",
    "log_pred = reg01.predict( scenario )\n",
    "y_pred = np.expm1( log_pred )\n",
    "print( '\\nPredicted Unit Sales: \\n{}'.format( round( y_pred, 0 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Case II: Binary Dependent Variable -- Logistic Regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Create your Data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer satisfaction is part of the DataFrame.  Satisfaction is measured on a five-point scale: *1 = Not at All Satisfied*, *5 = Very Satisfied*.  \n",
    "\n",
    "First, look at the frquency count of satisfaction.  But, there is a problem: you cannot use the same data as before since satisfaction is by customer and the data used so far are by transaction.  The satisfaction rating is in the customer DataFrame.  You need to first find the mean price and mean discounts by customer from the transactions DataFrame and then merge this new DataFrame with the customer DataFrame.  So, there are several steps:\n",
    "\n",
    "1. Extract the pocket price and discounts -- include the *CID*\n",
    "2. Group by the CID and calculate the means by *CID*\n",
    "3. Merge with the customer DataFrame\n",
    "4. Recode the scale values in the merged file so that 1 is the top-two values (called *top-two box* or *T2B*) and 0 is all other values.  The *T2B* is *Very Satisfied*.\n",
    "5. Train a model with *T2B* satisfaction as a function of the pocket price, discounts, and Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## ===> Step 1: Extract the pocket price and discounts -- include the CID\n",
    "##\n",
    "tmp = df[ [ 'CID', 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ] ]\n",
    "tmp.set_index( 'CID', inplace = True )\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 2: Group by the CID and calculate the means by CID\n",
    "##\n",
    "x = tmp.groupby( 'CID' ).mean()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 3: Merge with the customer DataFrame\n",
    "##\n",
    "df_sat = x.merge( df_cust, left_index = True, right_index = True )\n",
    "df_sat.head()\n",
    "df_sat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Do a quick check of the value distribution.\n",
    "##\n",
    "## Use the DataFrame's value_counts() method. Sort by the\n",
    "## scale values 1 - 5.\n",
    "##\n",
    "df_sat.buyerSatisfaction.value_counts( sort = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 4: Recode the scale values so that 1 is the top-two values \n",
    "## (called \"top-two box\" or \"T2B\") and 0 is all other values.  \n",
    "## The \"T2B\" is \"Very Satisfied\".\n",
    "##\n",
    "## Recode using Numpy's select function\n",
    "##\n",
    "## ===> Step 4.A: Define labels for the recoded values\n",
    "##\n",
    "lbl = [ 1, 0 ]\n",
    "##\n",
    "## ===> Step 4.B: Specify the conditions for the recoding\n",
    "##\n",
    "conditions = [\n",
    "    ( df_sat.buyerSatisfaction >= 4 ),\n",
    "    ( df_sat.buyerSatisfaction < 4 )\n",
    "]\n",
    "##\n",
    "## ===> Step 4.C: Do the recoding \n",
    "##\n",
    "df_sat[ 'sat_t2b' ] = np.select( conditions, lbl )\n",
    "##\n",
    "df_sat[ 'sat_t2b' ].value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model *T2B* satisfaction as a function of the pocket price and discounts.  First, create training and testing DataFrames as before but with *sat_t2b* as the *y* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 5: Train a model.\n",
    "##\n",
    "## Create the X and y data for splitting\n",
    "##\n",
    "y = df_sat[ 'sat_t2b' ]\n",
    "x = [ 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc', 'Region' ]\n",
    "X = df_sat[ x ]\n",
    "##\n",
    "## Split the data.  The default is 3/4 train.\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, \n",
    "                                                    random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data\n",
    "##\n",
    "print( 'Training Data: \\n{}'.format( X_train.head() ) ) \n",
    "print( \"\\n\" )\n",
    "print( 'Testing Data: \\n{}'.format( y_test.head() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two training sets for convenience\n",
    "##\n",
    "yy = pd.DataFrame( { 'sat_t2b':y_train } )\n",
    "train = yy.merge( X_train, left_index = True, right_index = True )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two testing sets for convenience\n",
    "##\n",
    "yy = pd.DataFrame( { 'sat_t2b':y_test } )\n",
    "test = yy.merge( X_test, left_index = True, right_index = True )\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color = black> Train a Model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Train a logit model\n",
    "##\n",
    "## ===> Step 1: Define a formula\n",
    "##\n",
    "formula = 'sat_t2b ~ Pprice + Ddisc + Odisc + Cdisc + Pdisc + C( Region )'\n",
    "##\n",
    "## ===> Step 2: Instantiate the logit model\n",
    "##\n",
    "mod = smf.logit( formula, data = train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model\n",
    "##\n",
    "logit01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model\n",
    "##\n",
    "print( logit01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Analyze the Results </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Import needed functions\n",
    "##\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "##\n",
    "## Make predictions\n",
    "##\n",
    "predictions = logit01.predict( test )\n",
    "predictions_nominal = [ 0 if x < 0.5 else 1 for x in predictions]\n",
    "print( classification_report( y_test, predictions_nominal, digits = 3 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, the count of **true negatives** ($tn$), **false negatives** ($fn$), **true positives** ($tp$), and **false positives** ($fp$) can be found from a *confusion matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a confusion matrix\n",
    "##\n",
    "x = confusion_matrix(y_test, predictions_nominal).ravel()\n",
    "##\n",
    "## zip the variable names and the confusion\n",
    "##\n",
    "lbl = [ 'tn', 'fp', 'fn', 'tp' ]\n",
    "##\n",
    "## display the zip matrix\n",
    "##\n",
    "from statsmodels.compat import lzip\n",
    "lzip( zip( lbl, x ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There were 4 true negatives, 23 false positives, 2 false negatives, and 72 true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Create labels\n",
    "##\n",
    "lbl = ['Not Satisfied', 'Satisfied']\n",
    "##\n",
    "## Create the confusion matrix\n",
    "##\n",
    "cm = confusion_matrix( y_test, predictions_nominal )\n",
    "tmp = pd.DataFrame(data=cm, index = lbl, columns = lbl )\n",
    "print( 'Confusion Matrix: \\n{}'.format( tmp ) )\n",
    "##\n",
    "## Plot the confusion matrix\n",
    "##\n",
    "sns.set( font_scale = 1.4 )   #for label size\n",
    "##\n",
    "ax = sns.heatmap( cm/cm.sum(), annot = True, annot_kws = { \"size\": 16 } )  # font size\n",
    "ax.set( title = 'Confusion Matrix of the Classifier', xlabel = 'Predicted',\n",
    "       ylabel = 'True' )\n",
    "ax.set_xticklabels(lbl)\n",
    "ax.set_yticklabels(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "75\\% of the cases were predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [ 'Model ' + str( i ) for i in range( 1, 2 ) ]\n",
    "##\n",
    "## Create a variable to hold the statistics to print; this is a dictionary.\n",
    "##\n",
    "info_dict = { '\\nn': lambda x: \"{0:d}\".format( int( x.nobs ) ),\n",
    "}\n",
    "##\n",
    "## Create the portfolio summary table.\n",
    "##\n",
    "summary_table = summary_col( [ logit01 ],\n",
    "            float_format = '%0.2f',\n",
    "            model_names = model_names,\n",
    "            stars = True, \n",
    "            info_dict = info_dict \n",
    ")\n",
    "summary_table.add_title( 'Summary Table for Living Room Blinds Sales' )\n",
    "print( summary_table )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Predicting with the Model </font>\n",
    "\n",
    "The prediction process is the same as discussed for *Case I* above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Case III: Constants - Decision Trees </font>\n",
    "\n",
    "Decision Trees can handle continuous or discrete dependent variables.  They are an alternative to *OLS* and logistic regression: you don't have to specify a \"model\" *per se*.  They also have the advantage that a visual display, a *tree*, is produced which is easier for management and clients to understand than complex regression output and statistics.  You will only look at a discrete case; a continuous case is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import decision tree classifier\n",
    "##\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import export_graphviz\n",
    "##\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "##\n",
    "## Convert \"Region\" to integers: the decision tree must have all numerics\n",
    "## Note 1: use the LabelEncoder function for this\n",
    "## Note 2: \"Region\" will be encoded in alphanumeric order:\n",
    "##\n",
    "##          0: Midwest\n",
    "##          1: Northeast\n",
    "##          2: South\n",
    "##          3: West\n",
    "##\n",
    "print( '\\nTraining Data before recoding Region:\\n \\n{}'.format( X_train.head() ) )\n",
    "le = preprocessing.LabelEncoder()\n",
    "X_train[ 'Region' ] = labelencoder.fit_transform( X_train[ 'Region'] )\n",
    "print( '\\nTraining Data after recoding Region:\\n \\n{}'.format( X_train.head() ) )\n",
    "##\n",
    "X_test = X_test.apply( le.fit_transform )\n",
    "print( 'Testing Data: \\n{}'.format( X_test.head() ) )\n",
    "##\n",
    "## Instantiate the tree\n",
    "##\n",
    "dtree = tree.DecisionTreeClassifier( random_state = 0, max_depth = 3, \n",
    "                                    min_samples_leaf = 5 )\n",
    "##\n",
    "## Fit the tree\n",
    "##\n",
    "dtree.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional packages are needed to plot a decision tree:\n",
    "\n",
    "- graphviz\n",
    "- pydotplus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Both packages may have to be installed before they can be used.  \n",
    "## Use the operating system to do this.\n",
    "##\n",
    "import os\n",
    "!{sys.executable} -m pip install graphviz\n",
    "!{sys.executable} -m pip install pydotplus\n",
    "##\n",
    "## Tell Python where the graphviz package is load; then load it.\n",
    "##\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "##\n",
    "## Load the following packages\n",
    "##\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Check Model Accuracy </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## The score attribute\n",
    "##\n",
    "print( \"Accuracy on training data: {:.3f}\".format( dtree.score( X_train, y_train )))\n",
    "print( \"Accuracy on testing data: {:.3f}\".format( dtree.score( X_test, y_test )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are good scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Display the Decision Tree </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import needed packages\n",
    "##\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "##\n",
    "## Displaying a tree is a slight challenge!\n",
    "## There are four steps:\n",
    "##\n",
    "## ===> Step 1: Create a placeholder for all the plotting points.\n",
    "##\n",
    "dot_data = StringIO()\n",
    "##\n",
    "## ===> Step 2: Extract the feature names for labels models\n",
    "##\n",
    "feature_names = [ i for i in X_train.columns ]\n",
    "##\n",
    "## ===> Step 3: Export the plotting data to the placeholder\n",
    "##\n",
    "export_graphviz(dtree, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,\n",
    "                class_names = [ 'Not Satisfied', 'Satisfied' ],\n",
    "                feature_names = feature_names ,\n",
    "                proportion  = True\n",
    "               )\n",
    "##\n",
    "## ===> Step 4: Create the display\n",
    "##\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note_**\n",
    "\n",
    "For the right node on the third level, \"$Region \\le 0.05$\" is interpreted as *Region* having a value less than or equal to 0.05.  Since only $Region = 0$ meets this criteria and $Region = 0$ is the Midwest, than if *Region* is the Midwest, go to the left; otherwise, go to the right for regions Northest, South, and West. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> Exercise \\#3.3 </font>\n",
    "\n",
    "Interpret the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> What's Next? </font>\n",
    "\n",
    "In Lesson 4, I will briefly discuss how to share your notebooks.  Recall that there are three reasons for using Jupyter notebooks in your analysis work:\n",
    "\n",
    "1. managing workflows;\n",
    "2. documenting analyses for reproducibility; and\n",
    "3. sharing results with colleagues.\n",
    "\n",
    "I'll discuss these in the next lesson.\n",
    "<br><br><br>\n",
    "<font color = red, size = \"+3\"><b> Five Minute Break </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
