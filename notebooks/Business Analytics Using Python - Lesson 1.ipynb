{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red>Introduction to Business Analytics:<br>Using Python for Better Business Decisions</font>\n",
    "=======\n",
    "<br>\n",
    "    <center><img src=\"http://dataanalyticscorp.com/wp-content/uploads/2018/03/logo.png\"></center>\n",
    "<br>\n",
    "Taught by: \n",
    "\n",
    "* Walter R. Paczkowski, Ph.D. \n",
    "\n",
    "    * My Affliations: [Data Analytics Corp.](http://www.dataanalyticscorp.com/) and [Rutgers University](https://economics.rutgers.edu/people/teaching-personnel)\n",
    "    * [Email Me With Questions](mailto:walt@dataanalyticscorp.com)\n",
    "    * [Learn About Me](http://www.dataanalyticscorp.com/)\n",
    "    * [See My LinkedIn Profile](https://www.linkedin.com/in/walter-paczkowski-a17a1511/)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About this Notebook\n",
    "-----------------------------\n",
    "\n",
    "This notebook accompanies the PDF presentation ***Business Analytics: Using Python for Better Business Decisions*** by Walter R. Paczkowski, Ph.D. (2019).  There is more content and commentary in this notebook than in the presentation deck.  Nonetheless, the two complement each other and so should be studied together.  Every effort has been made to use the same key slide titles in the presentation deck and this notebook which will help your studying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercisers and Solutions\n",
    "------------------------------------\n",
    "\n",
    "Exercises are interspersed throughout the lessons.  Their solutions are in a separate *Solutions Manual*.  There are also Appendices to some lessons that contain extra exercises.  You can view these as \"homework problems.\"  Their solutions are also in the *Solution Manual*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful Online Tutorials\n",
    "---------------------------------\n",
    "\n",
    "* <a href=\"http://docs.python.org/2/tutorial/\" target=\"_parent\">Python Tutorial</a>\n",
    "\n",
    "* <a href=\"https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html\" target=\"_parent\">Pandas Tutorial</a>\n",
    "\n",
    "* <a href=\"https://seaborn.pydata.org/tutorial.html\" target=\"_parent\">Seaborn Tutorial</a>\n",
    "\n",
    "* <a href=\"https://www.statsmodels.org/stable/index.html\" target=\"_parent\">Statsmodels Tutorial</a>\n",
    "\n",
    "\n",
    "Helpful/Must-Read Book\n",
    "-----------------------------------\n",
    "* <a href=\"https://www.amazon.com/gp/product/1491957662/ref=as_li_tl?ie=UTF8&tag=quantpytho-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1491957662&linkId=8c3bf87b221dbcd8f541f0db20d4da83\" target=\"_parent\">Main Pandas go-to book: </a> *Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython* (2nd Edition) by Wes McKinney.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue> Lesson \\#1:<br>Simple Analytics: Understanding and Preparing Your Data </font>\n",
    "\n",
    "In this lesson, you will learn:\n",
    "\n",
    "1. how to document your data;\n",
    "2. some fundamentals of Jupyter notebooks for documenting analysis workflow;\n",
    "3. how to import Python packages;\n",
    "4. how to import data into a Pandas DataFrame; and\n",
    "5. how to manipulate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> Documenting Your Data and Workflow: Best Practices </font>\n",
    "\n",
    "There are two things to do to document your work:  \n",
    "\n",
    "1. document your data; and\n",
    "2. document your workflow.\n",
    "\n",
    "A _Data Dictionary_ is used for the first and a _Jupyter notebook_ is used for the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Documenting Your Data </font>\n",
    "\n",
    "The first task in any data analysis is data documentation, perferably in the form of a *Data Dictionary*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Data Dictionary </font>\n",
    "\n",
    "A data dictionary contains *metadata* which are data about the data.  Metadata can be anything that helps you understand the data you're using.  Based on [Wikipedia](http://en.wikipedia.org/wiki/Metadata), metadata are information about the distinct data items, such as:\n",
    "\n",
    "1. means of creation;\n",
    "2. purpose of the data;\n",
    "3. time and date of creation;\n",
    "4. creator/author/keeper of the data;\n",
    "5. placement on a network (electronic form);\n",
    "6. where the data were created;\n",
    "7. what standards were used to create the data; and \n",
    "8. etc.\n",
    "\n",
    "I'll restrict the metadata to:\n",
    "\n",
    "1. Variable name;\n",
    "2. Possible values or value ranges;\n",
    "3. Source; \n",
    "3. Date received; and \n",
    "4. Mnemonic.\n",
    "\n",
    "The mnemonic is the label used in data files and statistical and modeling output.  \n",
    "\n",
    "| Variable                  | Values                                 | Source       | Date Received | Mnemonic     |\n",
    "|---------------------------|----------------------------------------|--------------|---------------|--------------|\n",
    "| Order Number              | Nominal Integer                        | Order Sys    | 02/01/2019    | Onum         |\n",
    "| Customer ID               | Nominal                                | Customer Sys | 02/01/2019    | CID          | \n",
    "| Transaction Date          | MM/DD/YYYY                              | Order Sys    | 02/01/2019    | Tdate        | \n",
    "| Product Line ID           | Five rooms of house                    | Product Sys  | 02/01/2019    | Pline        |\n",
    "| Product Class ID          | Item in line                           | Product Sys  | 02/02/2019    | Pclass       |\n",
    "| Units Sold                | Number of units per order              | Order Sys    | 02/01/2019    | Usales       |\n",
    "| Product Returned?         | Yes/No                                 | Order Sys    | 02/01/2019    | Return       |\n",
    "| Amount Returned           | Number of units                        | Order Sys    | 02/01/2019    | returnAmount |\n",
    "| Material Cost/Unit        | \\$US cost of material                  | Product Sys  | 02/01/2019    | Mcost        |\n",
    "| List Price                | \\$US list                              | Price Sys    | 02/01/2019    | Lprice       |\n",
    "| Dealer Discount           | \\% discount to dealer (decimal)        | Sales Sys    | 02/01/2019    | Ddisc        |\n",
    "| Competitive Discount      | \\% discount for competition (decimal)  | Sales Sys    | 02/01/2019    | Cdisc        |\n",
    "| Order Size Discount       | \\% discount for size (decimal)         | Sales Sys    | 02/01/2019    | Odisc        |\n",
    "| Customer Pickup Allowance | \\% discount for pickup (decimal)       | Sales Sys    | 02/01/2019    | Pdisc        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Documenting Your Work Flow </font>\n",
    "\n",
    "Documenting your workflow is as important as documenting your data.  This documentation will enable you to reproduce your work and make it easier for a colleague to follow what you did.  The *Jupyter notebook* paradigm is the best platform for this documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = black> Jupyter Notebooks: Overview </font>\n",
    "\n",
    "Jupyter is a sophisticated programming tool sometimes called an *ecosystem*.  It is an ecosystem because it will handle a number of programming languages, Python being one of them.  The langauges are called _kernels_.  Other kernels are R, Julia, Fortran to mention a few.  There are over 100 kernels supported by Jupyter.  Jupyter was originally created to handle Julia, Python, and R.  In fact, the name \"Jupyter\" is a contraction for **JU**lia, **PYT**hon, and **R**.\n",
    "\n",
    "The paradigm for Jupyter is a lab notebook used in the physical sciences for laboratory experiment documentation.  The Jupyter notebook consists of cells where text and programming code are entered and executed or *run*.  There are two basic cells:\n",
    "\n",
    "1. Code cell; and \n",
    "2. Markdown cell.\n",
    "\n",
    "A code cell is where programming code is entered and executed.  The results are displayed in another cell that appears immediately below a code cell.  To organize the code and result, the pair of cells are labeled with a marker for the code and the result.  The code marker is In[] and the result marker is Out[].  Inside the square brackets are numbers representing the sequence in which code is executed.  The numbers will vary, and even be out of order, if a code cell is executed several times but a following code cell is not reexecuted.  The sequencing numbers can be reset by rerunning the kernel.  This is done by clicking on *Kernel* on the main toolbar and selecting *Restart & Run All*.  \n",
    "\n",
    "The markdown cell is where documentation is entered.  This cell, in fact, is a markdown cell.  It is that you use as many markdown cells as possible to document your work.  You will see many examples of this below.<br>\n",
    "\n",
    "A code cell is the default.  You make a code cell a markdown cell by using the drop-down menu list on the main toolbar.  Of course, you can change a markdown cell to a code cell using the same drop-down list.  You could also use keyboard short-cuts.  See *Help/Keyboard Shortcuts* on the main menu bar. \n",
    "\n",
    "You can easily insert/delete cells and move them from one location in your notebook to another.  To insert a cell above the current cell, select the cell as the base for the insert and click on the colored bar on the left until it turns blue.  Then type *A* to insert a cell above the current cell.  Type *B* to insert one below it.  Type *DD* (two *D*'s) to delete the current cell.  To move a cell, select the cell you want to move, click the colored bar on the left, and use the up-arrow and down-arrow icons on the main menu toolbar.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = black> Importing Python Packages </font>\n",
    "\n",
    "Python is a powerful programming language that allows you to perform all standard programming operations in a clear and consistent manner.  Its strength, adhered to by Python programers, is a coding format that emphasizes readable code.  Indentation is the primary way to accomplish this.  Also, its strength is based on a very wide array of *packages* or *modules* or *libraries*.  Packages perform analysis or data manipulation operations.  There are many packages, each one providing a special set of analysis tools so a package can be viewed as a container of functions.  Sometimes a package contains smaller, more specialized packages so a grand package could be a container for smaller ones.  You will see how to access and use packages and subset packages in this and other lessons.\n",
    "\n",
    "Pandas is a data manipulation and graphing package with a lot of capabilities.  It will be used extensively in these lessons.  Seaborn is a scientific graphing package that is intuitive to use.  Although Pandas has visualization methods, Seaborn is preferred because of its quality, extent, and easier syntax.  Both packages use Matplotlib for base graphing functions.  Statsmodels has an array of statistical modeling functions, only a few of which will be used in these lessons.  Numpy and Matplotlib are base packages for Pandas, Seaborn, and Statsmodels.  Except for a few functions, Numpy and Matplotlib will not be used directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> Introduction to Python Packages </font>\n",
    "\n",
    "Analysis functions you will use, such as a function to create a graph or estimate a regression model, require that you specify the package they come from.  Otherwise, Python will not know where to look for the package.  You can either specify the whole package name or an alias to the package -- the alias is easier: there are fewer characters to type!\n",
    "\n",
    "I will use only a few packages in these lessons.  The packages and their typical, almost standard, aliases are:\n",
    "\n",
    "| Package      | Use                           | Typical Alias |\n",
    "|--------------|-------------------------------|---------------|\n",
    "| Pandas       | Basic data manipulation       | pd            |\n",
    "| Seaborn      | Scientific data visualization | sns           |\n",
    "| Statsmodels  | Regression analysis           | sm            |\n",
    "| Scikit-Learn | Machine Learning              | sci           |\n",
    "| Numpy        | Basic data manipulation       | np            |\n",
    "| Matplotlib   | Basic graphing library        | plt           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> Loading Packages </font>\n",
    "\n",
    "You have to load a package before you can use it.  Loading is done using an *import* command.  The alias is assigned when you import the package.  I recommend loading all the basic packages at once at the beginning of your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import standard packages\n",
    "##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "##\n",
    "## Import data visualization packages\n",
    "##\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "##\n",
    "## Matplotlib graphs can be drawn in the Jupyter notebook; the next \n",
    "## command says to do just that.\n",
    "##\n",
    "%matplotlib inline\n",
    "##\n",
    "## Set the seaborn grid style.  The dot between the seaborn alias,\n",
    "## \"sns\", and the set() function connects or \"chains\" the alias and the function.\n",
    "##\n",
    "sns.set()\n",
    "##\n",
    "## Set an option for the number of Pandas columns to display.  Eight in this case.\n",
    "## \n",
    "pd.set_option( 'display.max_columns', 8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanantion_**\n",
    "\n",
    "This code block loads the necessary Python packages for this course.  I recommend setting options, such as those for graphs and print, as was done here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> Accessing a Function in a Package </font>\n",
    "\n",
    "A function in a package can be accessed by telling Python the package where the function is located and, of course, the name of the function.  These two operations are done with one statement by *chaining* the package name and the function name.  The chain is formed by connecting the package name and the function name by inserting a dot between the two.  Usually, the package alias is used for improved readability.  An example of a chained command is:\n",
    "<br><br>\n",
    "***pd.read_csv( 'lesson1.csv' )***\n",
    "<br><br>\n",
    "where \"pd\" is the alias for Pandas and \"read_csv\" is a Pandas function that reads a *CSV* file (\"lesson1.csv\" in this example). Notice the dot(\".\") between the alias and the function name.  The dot is the chaining operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = black> Importing Your Data Into Pandas </font>\n",
    "\n",
    "Before you can begin any work, you must first import and examine the structure of your data.  This structure is a rectangular array or matrix or, in Pandas terminology, a *DataFrame*.  When you import your data using Pandas, the imported data immediately goes into a DataFrame.  This is very convenient because Seaborn and StatsModels functions recognize these DataFrames.\n",
    "\n",
    "Pandas provides a set of very flexible import functions.  Which one you should use depends on your data format.  Some typical formats and relevant functions are:\n",
    "\n",
    "| Data Format | Pandas Import Function |\n",
    "|----------|---------------|\n",
    "| CSV       | read_csv       |\n",
    "| Excel     | read_excel     |\n",
    "| Clipboard | read_clipboard |\n",
    "| SQL       | read_sql       |\n",
    "| JSON      | read_json      |\n",
    "| SAS       | read_sas       |\n",
    "\n",
    "Some of these will demonstrated below.\n",
    "\n",
    "I will first import a *CSV* formatted file.  The package alias must be \"chained\" with the *read_csv* import function, otherwise Python will not know where to find the read function. \n",
    "\n",
    "When you import data, you must always specify the file path so Pandas can find the file.  If the data file is in the same directory as the notebook, then a path is unnecessary since Pandas always begins a search in the same directory as the notebook.  Otherwise, you have to specify the path.\n",
    "\n",
    "An example of data import is shown below.  Several more are shown in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data path as shown here; keep the format *r'path'* if necessary.  Remember, if your data are in the same directory as your notebook, then a path is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Specify a path to the CSV data and import or read the data.\n",
    "##\n",
    "file = r'../data/orders.csv'\n",
    "##\n",
    "## Import the data.  The parse_dates argument says to \n",
    "## treat Tdate as a date object.\n",
    "##\n",
    "df_orders = pd.read_csv( file, parse_dates = [ 'Tdate' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanantion_**\n",
    "\n",
    "The CSV file is specified using the string literal: *r'../Data/furniture/final data files/orders.csv'*.  The *\"r\"* at the begiining of the string tells the Python interpreter to treat this string as raw text that is not to be changed.  Without the *\"r\"*, the interpreter would treat any backslashes as escape characters which would change the meaning of the string.  Even though forward slashes are used in this code block, it is good practice to avoid issues and use the *\"r\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = black> Cleaning and Wrangling Your Data </font>\n",
    "\n",
    "A best practice is to check your data after you import them.  Check:\n",
    "\n",
    "1. the first few records (i.e., the \"head\");\n",
    "2. the shape which is the number of rows and columns;\n",
    "3. the column names; and \n",
    "4. for missing values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 1: Display first few records.\n",
    "##\n",
    "df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The \"head\" method is attached to a DataFrame when you create it.  The default is to display the first five records.  You could use *n = 10* as an argument to display the first 10 records: *df.head( n = 10 )*.  You could display the last five records using the *tail* method.  The default is also five which could be changed as for the *head* method.  For both *head* and *tail*, the number of columns displayed is set using *pd.set_option( 'display.max_columns', 8 )* as was done in the package loading section above.  The *head* and *tail* methods are chained to the DataFrame name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 2: Check the shape of the data\n",
    "##\n",
    "df_orders.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "*shape* is an attribute of the DataFrame so it does not require parentheses; it does not have any arguments.  Functions and methods have arguments (which may be defaults) so parentheses are required.  The *shape* attribute is chained to the DataFrame name.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The *shape* attribute returns the number of rows and the number of columns in that order.  The *orders* DataFrame has 70,270 *rows* or *records* or *observations* and 14 *columns* or *variables* or *features*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 3: Check the column names\n",
    "##\n",
    "df_orders.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "*columns* is an attribute of the DataFrame.\n",
    "\n",
    "**_Interpretaion_**\n",
    "\n",
    "When checking the column names, be sure there are no white spaces before and after the name.  White spaces can (and will) cause problems because your tendency will be to write a column name without the leading and trailing white spaces; Python will then not recognize the name.  If you see leading and trailing white spaces, you can remove them using the following:\n",
    "\n",
    "*df_orders.columns = df_orders.columns.str.strip()*\n",
    "\n",
    "where *str* is the string package which is part of the base Python kernel and automatically loaded with Python.  You may also want to convert the column names to all lower case:\n",
    "\n",
    "*df_orders.columns = df_orders.columns.str.lower()*\n",
    "\n",
    "You could do both at once using:\n",
    "\n",
    "*df_orders.columns = df_orders.columns.str.strip().str.lower()*\n",
    "\n",
    "Notice the use of *str* twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 4: Check for Missing\n",
    "##\n",
    "df_orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "*info* is a method chained to the DataFrame.  It returns the number of non-missing records for each column plus data types: object (i.e., text string), floating point numbers, integers, and datetime.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "There are 14 columns with 10 having 70,270 nonmissing values while the last four have less than 70,270 so they have mising values.  For example, *Ddisc* (the dealer discount based on the Data Dictionary) has 70,262 records so 8 are missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = black> Manipulating Your Data </font>\n",
    "\n",
    "Sometimes you need to clean the columns in your DataFrame.  The columns are *variables* or *features*.  This could be deleting, printing/displaying, and creating new variables.  Other times you have to *merge* or *join* your DataFrame with another DataFrame to have a more complete data set for your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> Creating Variables </font>\n",
    "\n",
    "You can create new variables using standard arithmetic notation.  Here is an example of adding the four discounts to get the total discount.  Each discount variable is an *attribute* of the DataFrame and so they can be accessed using dot notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate total discount.\n",
    "##\n",
    "## Discounts are sometimes called \"leakages\" so the total is \n",
    "## the total leakage.\n",
    "##\n",
    "## Note: use \"axis = 1\" in the sum() function to sum across columns.\n",
    "## This allows you to do the summation even with missing values.\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ 'Tdisc' ] = df_orders[ x ].sum( axis = 1 )\n",
    "##\n",
    "## Display only the discounts\n",
    "##    Create a list of what to print. \n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc', 'Tdisc' ]\n",
    "df_orders[ x ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanantion_**\n",
    "\n",
    "The *sum* method has an axis argument that specifies the axis the function is to be applied on.  *axis = 0* specifies summing along the rows for each column (i.e., sum down a column) and *axis = 1* specifies summing along the columns in each row.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "Notice the *NaN* values.  *NaN* stands for *Not a Number*.  These are missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue> Exercises </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Exercise \\#1.1 </font>\n",
    "\n",
    "The *Pocket Price* is the list price less total discounts or total leakages.  It is the amount the business \"pockets\" and is the amount the customer actually pays.  The pocket price formula is $Pprice = Lprice \\times (1  - Tdisc)$.  Calculate the pocket price and display the first five records for the list price and pocket price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> Merge or Join DataFrames </font>\n",
    "\n",
    "It is not unusual to have data in two (or more) tables so you will need to merge or join them to get all the data you need for an analysis.  For our problem, a second data table has information on each customer and this second table must be merged with the orders table.  The merge is done on the customer *ID* (*CID*).  There are many types of joins but we will only use an *inner join* in the examples.  *Inner join* is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Import a second DataFrame on the customers\n",
    "##\n",
    "file = r'../data/customers.csv'\n",
    "df_cust = pd.read_csv( file )\n",
    "df_cust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Do an inner join using CID as the link\n",
    "##\n",
    "df = pd.merge( df_orders, df_cust, on = 'CID' )\n",
    "##\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The merge function takes two arguments: the *left* table and the *right* table to merge or join.  The tables are in that order.  A third argument specifies what to merge on.  There are several options for the *on* variable.  In this example, the *on* variable is just the common key in each table: *CID*.\n",
    "\n",
    "An alternative form for the merge statement is:\n",
    "\n",
    "*df = df_orders.merge( df_cust, on = 'CID' )*\n",
    "\n",
    "An *inner join* is the default.  \n",
    "\n",
    "See the Pandas documentation <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html\" target=\"_parent\">here</a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\" target=\"_parent\">here</a> for extensive discussion with examples about this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Check the shape of the new DataFrame against \n",
    "## that of the orders and customers DataFrames\n",
    "##\n",
    "print( \"Shape of the orders DataFrame: {}\\n\".format( df_orders.shape ) )\n",
    "print( \"Shape of the customers DataFrame: {}\\n\".format( df_cust.shape ) )\n",
    "print( \"Shape of the new DataFrame: {}\".format( df.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "A variation on the print function is used: the *format* command.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "*df_orders* has 16 columns, *df_cust* has 7 columns, while the merged *df* has 22.  The difference of one column is the *CID* which is in both and is the linking variable; it's only included once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue> Exercises </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Exercise \\#1.2 </font>\n",
    "\n",
    "Merge your order DataFrame and the customer DataFrame.  Name the merged DataFrame *df*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = black> Summary Statistics for Your Data </font>\n",
    "\n",
    "Summary statistics are a mainstay for starting any analysis.  Pandas has all the usual descriptve statistics.  One function, *describe()* will display the essential ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: \"describe\" is a method attached to the DataFrame so it requires ().\n",
    "## Round to 1 decimal place for readability (more decimal places are\n",
    "## unnecessay, anyway).\n",
    "##\n",
    "## Display the descriptive statistics for the discounts.\n",
    "##    First create a list of variables to display\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc', 'Tdisc' ]\n",
    "df[ x ].describe().round( 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *round* function is chained to the *describe* method.  An alternative way to round is shown next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above report for the descriptive statistics is a challenge to read.  I prefer to have the statistics as the columns.  This can easily be done once you recognize that the report is just a matrix.  Matrices can be transposed which could help you read the report more easily.  Use the \"*T*\" attribute to transpose a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example of transposed matrix and alternative\n",
    "## round function use.\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc', 'Tdisc' ]\n",
    "round( df[ x ].describe().T, 1 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "From the *Five Number Summary* (min/25%/50%/75%/max), you can determine the skewness of your data.\n",
    "\n",
    "1. Symmetric: $(75\\% - 50\\%) = (50\\% - 25\\%)$\n",
    "2. Right Skewed: $(75\\% - 50\\%) > (50\\% - 25\\%)$\n",
    "3. Left Skewed: $(75\\% - 50\\%) < (50\\% - 25\\%)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** What is the skewness for the Dealer Discount (*Ddisc*)?\n",
    "\n",
    "**ANSWER**: right skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue> Exercises </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = black> Exercise \\#1.3 </font>\n",
    "\n",
    "Using your merged orders/customers DataFrame, *df*, create a summary statistics display.  What is the skewness of the Total Discount (Tdisc)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = black> What's Next? </font>\n",
    "\n",
    "In Lesson 2, I will show you how to do some basic graphing or *visualization* of your data.  This may seem more like scientific visualization than business visualization.  The latter is usually *infographics* which is not useful for gaining insight and, hence, useful Rich Information.  The former, scientific visualization, is the tool for extracting Rich Information. \n",
    "<br><br><br>\n",
    "<font color = red, size = \"+3\"><b> Five Minute Break </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Appendix</font>\n",
    "\n",
    "This Appendix contains material extra to this lesson, material that you may want to review to solidify your understanding and knowledge about working with Python and Pandas.\n",
    "\n",
    "This Appendix covers:\n",
    "\n",
    "1. Importing data into Pandas;\n",
    "2. Checking your data;\n",
    "3. Manipulating columns of a Pandas DataFrame; and doing\n",
    "4. Correlation Analysis.\n",
    "\n",
    "Homework exercises with solutions (in the Solutions Manual) are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1.1: Different Ways to Import Data Into Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also define the path without the file name if you plan to use multiple files from the same directory.  You just have to define the general path and then concatenate the file name.  Here is an example.  Note:\n",
    "\n",
    "1. the \"+\" which instructs Pandas to add or concatenate the two strings;\n",
    "2. the quotes around the file name; and\n",
    "3. the forward slash as the last symbol in the path definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify a path to the CSV data and concatenate the file name.\n",
    "##\n",
    "## Not run\n",
    "##\n",
    "## path = r'../data/'\n",
    "## df_orders = pd.read_csv( path + 'orders.csv' )\n",
    "## df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following form if your data are in the same directory as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Do not specify a path if the CSV file is in the same directory\n",
    "## as your notebook.\n",
    "##\n",
    "## Not run\n",
    "##\n",
    "## df_orders = pd.read_csv( 'orders.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data are in an Excel file, use *pd.read_excel( filename, 'sheetname' )*.  The sheet name could be a character string or a sheet number as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: Import an Excel file\n",
    "##\n",
    "## Not run\n",
    "##\n",
    "## path = r'../data/'\n",
    "## df_orders = pd.read_excel( path + 'orders.xlsx', 'furniture' )\n",
    "## df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1.2: Some Additional Information on Checking Your Data\n",
    "\n",
    "Look at:\n",
    "\n",
    "1. the head of your data;\n",
    "2. the shape of your DataFrame; \n",
    "3. a list of column names; and\n",
    "4. the missingness of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task \\#1: Display the first few records of your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: list the head of the imported data\n",
    "## n = 5 is the default\n",
    "##\n",
    "df_orders.head( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should immediately notice that several discounts have missing values indicated by *NaN* (*Not a Number*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task \\#2: Check the shape of your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: check the shape of the imported data\n",
    "## Note 1: the order of the returned shape is always #rows, #columns\n",
    "## Note 2: there is no () for this command because the shape\n",
    "## is a DataFrame attribute\n",
    "##\n",
    "print( \"Shape of the DataFrame: {}\".format( df_orders.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70,270 rows or observations and 15 columns or variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task \\#3: Check the column names in your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: list the column names\n",
    "##\n",
    "df_orders.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task \\#4: Check for missing data in your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Use the DataFrame's information content.  The info()\n",
    "## method returns the number of non-missing rows for\n",
    "## each variable.  The numbers should be the same.\n",
    "##\n",
    "df_orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above listing indicates that the four discounts each have missing values. \n",
    "\n",
    "You can count the number of missing values using the *isnull()* method and chaining the *sum()* function.  The *isnull()* method returns a Boolean variable so the *sum()* function just adds 0 and 1 values.  Use a nice print statement for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Sum the Boolean variable returned by isull()\n",
    "## Let us check the Order Discount (Odisc).\n",
    "##\n",
    "x = df_orders.Odisc.isnull().sum()\n",
    "print( 'Missing count for Odisc: {}'.format( x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the proportion of each variable that is missing rather than the sum. Proportions are more meaningful. Use the *mean()* function for this.  Since *isnull()* returns a Boolean, the mean is just the proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check for missing values only for the discounts.\n",
    "## Chain the mean() function to the isnull() method.\n",
    "## Note: Do this for first 20 records for illustration only.\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ x ].iloc[ :20 ].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a heatmap of missing data using the *isnull()* method on the entire DataFrame.  Use the transpose attribute, *T*, for a more readable chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Note: the \"cbar = False\" argument turns off the color bar\n",
    "## Note: Do this for first 20 records for illustration only\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "ax = sns.heatmap( df_orders[ x ].iloc[ :20, : ].isnull().T, cbar = False )\n",
    "ax.set( title = 'Heatmap of Missing Data' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1.3: Miscellaneous Pandas DataFrame Column Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Deleting Columns </font>\n",
    "\n",
    "You can easily delete unwanted columns with the *drop* method.  You must specify if you want the DataFrame replaced or not; the default is to not replace in which case you must assign a new name to the modified DataFrame.  If you drop a column, it is good practice to not replace your DataFrame so that you preserve your original data.\n",
    "\n",
    "The *drop* method can be used to drop rows or columns, so you have to tell it which one.  This is done with the *axis* arugment.  The DataFrame, as a simple rectangular array, is said to have two *axes*: the row axis and the column axis.  Since in mathematics the size of a matrix is conventionally specified as $\\#row \\times \\#columns$ (rows always come before columns), the DataFrame axes are designated as 0 and 1 for rows and columns, respectively, since 0 comes before 1.  Specifying *axis = 0* in the *drop* method says to drop a row while specifying *axis = 1* says to drop a column. The default is *axis=0*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drop a column and replace the DataFrame.  Notice that axis = 1 is used\n",
    "## to drop a column.\n",
    "## \n",
    "## Not Run\n",
    "##\n",
    "## df_orders.drop( 'obs', axis = 1, inplace = True )\n",
    "## df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Printing or Displaying Specific Columns </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print specific columns by referring to their column indexes.  You can also print rows by chaining the *head()* function to the DataFrame name.  The default is the first five rows of your selection.\n",
    "\n",
    "In this example, I will print the specific columns 2, 3, and 4.  What you would count as columns 2 - 4, Python counts as 1 - 3 because of the offset from the beginning.  Note that the column listing is left inclusive but right exclusive; in math notation, it is ( x ].  So telling Python to print columns 1 - 3 is done with *columns[ 1:4 ]* which says to include column 1 and up to but excluding column 4.  Remember that column 1 is the first offset from the beginning so you see it as the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: \n",
    "##\n",
    "## Notice that columns 1, 2, and 3 BUT NOT 4\n",
    "## are displayed\n",
    "##\n",
    "df_orders[ df_orders.columns[ 1:4 ] ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a list, which is in square brackets, to select the columns and then use that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a list of columns\n",
    "##\n",
    "## Not Run\n",
    "##\n",
    "##x = [ 'Tdate', 'State', 'Region' ]\n",
    "##df_orders[ x ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Creating Variables </font>\n",
    "\n",
    "The following are exercises to create variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Exercise \\#A 1.3.1 </font>\n",
    "\n",
    "Calculate total revenue as $Rev = Usales \\times Pprice$.  Use the df_orders DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Exercise \\#A 1.3.2 </font>\n",
    "\n",
    "*Contribution* and *contribution margin* are two values financial analysts often examine.  Contribution is comparable to what economists call *profit* but is more restricted in that it just refers to a product without considering any fixed or overhead costs.  Contribution is $Con = Revenue - Material~Cost$ and contribution margin is $CM = \\dfrac{Con}{Revenue}$.  Calculate both quantities and display the first 5 records of unit sales, pocket price, material cost, revenue, contribution, and contribution margin.  Use the df_orders DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Exercise \\#A 1.3.3 </font>\n",
    "\n",
    "Some products are returned so another revenue number, *revenue net of returns*, is more meaningful and revealing for business decisions.  Net revenue is\n",
    "<br><br>\n",
    "$Net Revenue = (Unit Sales - Returns) \\times Pocket Price$.\n",
    "<br><br>\n",
    "Calculate net revenue and call it 'netRev'.  Also calculate the loss in revenue due to the returns and call it 'lostRev'.  Display the first five records of the DataFrame using just gross revenue, net revenue, and the lost revenue due to returns.  Use the *df_orders* DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = black> Exercise \\#A1.3.4 </font>\n",
    "\n",
    "Display the descriptive statistics for gross revenue, net revenue, contribution, and contribution margin.  Round the answers to two decimal places.  Use the *df_orders* DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1.4: Correlation Analysis\n",
    "\n",
    "There is a *correlation* method attached to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: display a correlation matrix of the discounts\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "df[ x ].corr().round( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The correlation matrix has a 1.0 in the cells along its main diagonal (the diagonal running from top left to bottom right).  The off-diagonal cells have the pair-wise correlations.  Notice that the correlation matrix is symmetric around the main diagonal: the top portion (called the *upper triangle*) matches the bottom portion (called the *lower triangle*).\n",
    "\n",
    "Round to three decimal places is usually sufficient.\n",
    "\n",
    "The correlations are all very low indicating that the discounts are not linearly associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *heatmap* is sometimes more effective for displaying a correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: plot the correlation matrix as a heatmap\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "cor = df[ x ].corr()\n",
    "ax = sns.heatmap( cor )\n",
    "ax.set( title = 'Heatmap of the Correlation Matrix' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The cells along the main diagonal are all white which, by the color bar on the right, indicates they are all 1.0 as they should be.  All other cells are black indicating that the correlations are all 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = black> Exercise \\#A 1.4.1 </font>\n",
    "\n",
    "Create a correlation matrix and corresponding heatmap for gross revenue, net revenue, contribution, and contribution margin.  What would you tell your product manager about the correlations?  Use the *df_orders* DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
